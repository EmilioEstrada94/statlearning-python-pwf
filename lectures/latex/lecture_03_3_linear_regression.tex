\documentclass[mathserif, aspectratio=169]{beamer}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% need to split the includes to make spell checking work.
\input{preamble}
\subtitle{\bfseries%
  {Linear Regression, Part 3}\\%
  {\tiny\it qualitative predictors, interaction \& non-linear extensions, outliers, high leverage points}\\%
}
\begin{document}
\input{common}

\begin{frame}{Abstract}

	\begin{blurb}
		Linear models are an important topic in statistical learning.  

		The true relationships between predictors and responses are rarely linear.
		But linear models often provide reasonable approximation. They provide
		high interpretability and have low variance, mitigating the risk of over-fitting.
		Linear models can be extended to include (some) non-linear relationships. 

		Linear models also provide an excellent baseline to compare other models against: if 
		our sophisticated model does not do much better than a linear model we might consider
		trading some bias for lower variance.
	\end{blurb}
\end{frame}

\begin{frame}{Overview}
	\begin{itemize}
		\item Qualitative versus quantitative predictors.
		\item Interactions among predictors.
		\item Non-linear extensions to the linear model.
		\item Outliers. 
		\item High leverage points.
	\end{itemize}
	\bottomline{This will conclude our long journey through linear regression.}
\end{frame}

\begin{frame}{Example: Credit Data Set}
	\begin{columns}
		\begin{column}{0.5\textwidth}
			\begin{itemize}
				\item The plot on the right shows the \e{quantitative} variables
					in the data set.
				\item The dataset also contains \e{qualitative} predictors:
				\item[] \dat{gender}: \val{Male}, \val{Female}
				\item[] \dat{married}: \val{Yes}, \val{No}
				\item[] \dat{student}: \val{Yes}, \val{No}
				\item[] \dat{ethnicity}: \val{Asian}, \val{African American}, \val{Caucasian}
			\end{itemize}
		\end{column}
		\begin{column}{0.5\textwidth}
			\vspace{-10mm}
			\begin{center}
				\includegraphics[width=0.95\textwidth]{3_6s}
			\end{center}
		\end{column}
	\end{columns}
	\bl{We need to somehow \e{encode} the qualitative predictors.}
\end{frame}

\begin{frame}{Qualitative Predictors with Two Levels}
	\begin{itemize}
		\item The variables \dat{gender}, \dat{student} and \dat{married} are qualitative
			predictors with two levels.
		\item Suppose we are interested in whether \dat{gender} influences \dat{balance}.
		\item We can define a \e{dummy variable} to encode the gender:
			\[
				x_i =
				\begin{cases}
					1 & \text{if $i$th person is \val{Female}} \\
					0 & \text{if $i$th person is \val{Male}} \\
				\end{cases}
			\]
		\item This results in the model
			\[
				y_i = \beta_0 + \beta_1 x_i + \epsilon_i =
				\begin{cases}
					\beta_0 + \beta_1 + \epsilon_i & \text{if $i$th person is \val{Female}} \\
					\beta_0 + \epsilon_i & \text{if $i$th person is \val{Male}} \\
				\end{cases}
			\]
	\end{itemize}
	\bl{The distinction is important for the interpretation of the model.}
\end{frame}

\begin{frame}{Qualitative Predictors with Two Levels}
	\begin{popblock}{0.8\textwidth}{Model Interpretation}
			\begin{align*}
				\beta_0 &: \text{average \dat{{\tiny balance}} among males}\\
				\beta_0 + \beta_1 &: \text{average \dat{{\tiny balance}} among females}\\
				\beta_1 &: \text{average \dat{{\tiny balance}} difference between females and males}\\
			\end{align*}
	\end{popblock}
	\begin{popblock}{0.8\textwidth}{Fit Result}
		\begin{tabular}[h]{lrrrr}
			{} & {\blue Coefficient} & {\blue Std. Error} & {\blue $t$-statistic} & {\blue $p$-value} \\
			\dat{Intercept} & $509.80$ & $33.13$ & $15.389$ & $< 0.0001$ \\
			\dat{gender[Female]} & $19.73$ & $46.05$ & $0.429$ & $0.6690$ \\
		\end{tabular}
	\end{popblock}
	\bl{The observed average difference of \$19.73 is \e{not} significant.}
\end{frame}

\begin{frame}{Qualitative Predictors with Two Levels}
	\begin{itemize}
		\item We can also encode the gender differently:
			\[
				x_i =
				\begin{cases}
					1 & \text{if $i$th person is \val{Female}} \\
					-1 & \text{if $i$th person is \val{Male}} \\
				\end{cases}
			\]
		\item This results in the model
			\[
				y_i = \beta_0 + \beta_1 x_i + \epsilon_i =
				\begin{cases}
					\beta_0 + \beta_1 + \epsilon_i & \text{if $i$th person is \val{Female}} \\
					\beta_0 - \beta_1 + \epsilon_i & \text{if $i$th person is \val{Male}} \\
				\end{cases}
			\]
	\end{itemize}
	\bl{This leads to a different interpretation of the coefficients.}
\end{frame}

\begin{frame}{Qualitative Predictors with Two Levels}
	\begin{popblock}{0.8\textwidth}{Model Interpretation}
			\begin{align*}
				\beta_0 &: \text{overall average \dat{{\tiny balance}}, disregarding gender}\\
				\beta_1 &: \text{amount  above (below) average for females (males)}\\
			\end{align*}
	\end{popblock}
	\begin{popblock}{0.8\textwidth}{Fit Result}
		\begin{tabular}[h]{lrrrr}
			{} & {\blue Coefficient} & {\blue Std. Error} & {\blue $t$-statistic} & {\blue $p$-value} \\
			\dat{Intercept} & $519.67$ & $23.03$ & $22.569$ & $< 0.0001$ \\
			\dat{gender} & $9.87$ & $23.03$ & $0.429$ & $0.6690$ \\
		\end{tabular}
	\end{popblock}
	\bl{Note that the fit is essentially the same as before.}
\end{frame}

\begin{frame}{Qualitative Predictors with more than Two Levels}
	\begin{itemize}
		\item The \dat{ethnicity} variable has three possible values.
		\item In this case we need two dummy variables for the encoding, $x_{i1}$ and $x_{i2}$. 
		\item We choose the value \val{African American} as the \e{baseline}:
			\[
				x_{i1} =
				\begin{cases}
					1 & \text{if $i$th person is \val{Asian}} \\
					0 & \text{if $i$th person is not \val{Asian}} \\
				\end{cases}
				\;\;\;\;
				x_{i2} =
				\begin{cases}
					1 & \text{if $i$th person is \val{Caucasian}} \\
					0 & \text{if $i$th person is not \val{Caucasian}} \\
				\end{cases}
			\]
		\item This results in the model
			\[
				y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \epsilon_i =
				\begin{cases}
					\beta_0 + \beta_1 + \epsilon_i & \text{if $i$th person is \val{Asian}} \\
					\beta_0 + \beta_2 + \epsilon_i & \text{if $i$th person is \val{Caucasian}} \\
					\beta_0 + \epsilon_i & \text{if $i$th person is \val{African American}} \\
				\end{cases}
			\]
	\end{itemize}
	\bl{The choice of the baseline is arbitrary.}
\end{frame}

\begin{frame}{Qualitative Predictors with more than Two Levels}
	\begin{popblock}{0.8\textwidth}{Model Interpretation}
			\begin{align*}
				\beta_0 &: \text{average \dat{{\tiny balance}} among African Americans}\\
				\beta_1 &: \text{average \dat{{\tiny balance}} difference between African Americans and Asians}\\
				\beta_2 &: \text{average \dat{{\tiny balance}} difference between African Americans and Caucasians}\\
			\end{align*}
	\end{popblock}
	\vspace{-8mm}
	\begin{popblock}{0.8\textwidth}{Fit Result}
		\begin{tabular}[h]{lrrrr}
			{} & {\blue Coefficient} & {\blue Std. Error} & {\blue $t$-statistic} & {\blue $p$-value} \\
			\dat{Intercept} & $531.00$ & $46.32$ & $11.464$ & $< 0.0001$ \\
			\dat{ethnicity[Asian]} & $-18.69$ & $65.02$ & $-0.287$ & $0.7740$ \\
			\dat{ethnicity[Caucasian]} & $-12.50$ & $56.68$ & $-0.221$ & $0.8260$ \\
		\end{tabular}
	\end{popblock}
	\vspace{-3mm}
	\bl{We always need one less dummy variable than there are levels.}
\end{frame}

\begin{frame}{Extensions of the Linear Model}
	\begin{itemize}
		\item The standard linear regression model provides easily interpretable results.
		\item It also works well on many real world problems (event though the true\\
			relationships are rarely linear).
		\item However, it has the restrictive assumptions of \e{additivity} and \e{linearity}:
			\begin{cpage}
				\begin{enumerate}
					\item {\orange Additive assumption}: The effect of a predictor $X_j$ on the response $Y$
						is independent of the other predictors.
					\item {\orange Linearity assumption}: The change in the response $Y$ due to a one-unit 
						change in $X_j$ is constant.
				\end{enumerate}
			\end{cpage}
	\end{itemize}
	\bl{We explore some ways to weaken these assumptions while keeping the model linear.}
\end{frame}

\begin{frame}{Removing the Additive Assumption}
	\begin{itemize}
		\item We explore the idea of \e{interaction terms} using the \dat{Advertising} data set.  
		\item In particular, we want to check whether there is some synergy between\\
			the \dat{TV} and \dat{radio} budgets.
		\item Recall the form of the additive model only including the \e{main effects}:
			\[
				Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon
			\]
			\[
				\mdat{sales} = \beta_0 + \beta_1\times\mdat{TV} + \beta_2\times\mdat{radio} + \epsilon
			\]
		\item We now drop the additive assumption and introduce an interaction term:
			\[
				Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + {\blue \beta_3 x_1 x_2} + \epsilon
			\]
			\[
				\mdat{sales} = \beta_0 
				+ \beta_1\times\mdat{TV} 
				+ \beta_2\times\mdat{radio} 
				+ {\blue \beta_3\times\mdat{TV}\times\mdat{radio}} + \epsilon
			\]
	\end{itemize}
	\bl{This model is still linear in the parameters $\bm{\beta}$. Always include the \e{main effects}.}
\end{frame}

\begin{frame}{Removing the Additive Assumption}
	\begin{itemize}
		\item We can write the model in a slightly different form.
			\begin{align*}
				Y &= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2+ \epsilon \\
				{} &= \beta_0 + {\blue (\beta_1 + \beta_3 x_2)} x_1 + \beta_2 x_2 + \epsilon \\
				{} &= \beta_0 + {\blue \widetilde{\beta}_1} x_1 + \beta_2 x_2 + \epsilon \\
			\end{align*}
			\vspace{-12mm}
			\begin{align*}
				\mdat{sales} &= \beta_0 
				+ \beta_1\times\mdat{TV} 
				+ \beta_2\times\mdat{radio} 
				+ \beta_3\times\mdat{TV}\times\mdat{radio} + \epsilon\\
				{} &= \beta_0
				+ {\blue (\beta_1 + \beta_3\times\mdat{radio})}\times\mdat{TV} 
				+ \beta_2\times\mdat{radio} 
				+ \epsilon\\
				{} &= \beta_0
				+ {\blue\widetilde{\beta}_1}\times\mdat{TV} 
				+ \beta_2\times\mdat{radio} 
				+ \epsilon\\
			\end{align*}
		\vspace{-12mm}
		\item We can now interpret $\beta_3$ as the increase of the influence of \dat{TV} due to \dat{radio} 
			(or vice versa).
	\end{itemize}
	\bl{Remember we do not make formal claims of causality.}
\end{frame}

\begin{frame}{Removing the Additive Assumption}
	\begin{popblock}{0.8\textwidth}{Fit Result ($R_\text{interaction}^2 = 0.968,\; R_\text{additive}^2 = 0.897$)}
		\begin{tabular}[h]{lrrrr}
			{} & {\blue Coefficient} & {\blue Std. Error} & {\blue $t$-statistic} & {\blue $p$-value} \\
			\dat{Intercept} & $6.7502$ & $0.248$ & $27.23$ & $< 0.0001$ \\
			\dat{TV} & $0.0191$ & $0.002$ & $12.70$ & $< 0.0001$ \\
			\dat{radio} & $0.0289$ & $0.009$ & $3.24$ & $0.0014$ \\
			$\mdat{TV}\times\mdat{radio}$ & $0.0011$ & $0.000$ & $20.73$ & $< 0.0001$ \\
		\end{tabular}
	\end{popblock}
	\bl{There is strong evidence of synergy between the two budgets.}
\end{frame}

\begin{frame}{Interactions with Qualitative Predictors}
	\begin{itemize}
		\item Interaction terms with qualitative predictors are also possible.
		\item They even have a particularly nice interpretation.
		\item We return to the \dat{Credit} data set to examine this.
		\item Suppose we want to predict \dat{sales} from \dat{income} (quantitative) and
			\dat{student} (qualitative):
			\begin{align*}
				\mdat{balance}_i &\approx \beta_0 + \beta_1\times\mdat{income}_i +
				\begin{cases}
					\beta_2 & \text{if $i$th person is a student} \\
					0 & \text{if $i$th person is not a student}
				\end{cases}\\
				&= \beta_1\times\mdat{income}_i +
				\begin{cases}
					\beta_0 + \beta_2 & \text{if $i$th person is a student} \\
					\beta_0 & \text{if $i$th person is not a student}
				\end{cases}\\
			\end{align*}
	\end{itemize}
	\bl{This amounts to fitting two parallel lines: one for students and one for non-students.}
\end{frame}

\begin{frame}{Interactions with Qualitative Predictors}
	\begin{itemize}
		\item We want to allow for the effect of \dat{income} to change, depending on whether\\
			the value of \dat{student} is \val{Yes} or \val{No}.
		\item We therefore introduce an interaction term and the model becomes:
			\begin{align*}
				\mdat{balance}_i &\approx \beta_0 + \beta_1\times\mdat{income}_i +
				\begin{cases}
					\beta_2 + \beta_3 & \text{if $i$th person is a student} \\
					0 & \text{if $i$th person is not a student}
				\end{cases}\\
				&= 
				\begin{cases}
					(\beta_0 + \beta_2) + (\beta_1 + \beta_3)\times\mdat{income}_i & \text{if $i$th person is a student} \\
					\beta_0  + \beta_1\times\mdat{income}_i & \text{if $i$th person is not a student}
				\end{cases}\\
			\end{align*}
	\end{itemize}
	\bl{The lines are no longer parallel: the slope now depends on the value of \dat{student}.}
\end{frame}

\begin{frame}{Interactions with Qualitative Predictors}
	\vspace{-10mm}
	\begin{center}
		\includegraphics[width=0.8\textwidth]{3_7}
		 
		{\blue Left}: additive model, {\blue Right}: model with interaction term.
	\end{center}
	\bl{In the model with interaction the slopes are different.}
\end{frame}

\begin{frame}{Non-linear Relationships}
	\begin{itemize}
		\item We now look into the assumption that the relationship between the predictors\\
			$X$ and the response $Y$ is linear.
		\item We already stressed many times that this is in general \e{not} true.
		\item A common approach to address this is to add non-linear functions of one or more predictors 
			to the model design.
		\item In principle, this can be \e{any} function, of \e{any} number of predictors.
		\item Often we restrict ourselves to \e{polynomial} functions.
		\item It is important to note that the model is still linear in the parameters $\beta$.
	\end{itemize}
	\bl{This idea extends beyond linear models and is sometimes called \e{feature engineering}.}
\end{frame}

\begin{frame}{Non-linear Relationships}
	\begin{itemize}
		\item We illustrate this on the \dat{Auto} data set.
		\item We introduce a quadratic term in an attempt to better predict \dat{mpg} from
			\dat{horsepower}:
			\[
				\mdat{mpg} = \beta_0 + \beta_1\times\mdat{horsepower} 
				+ {\blue \beta_2\times\mdat{horsepower}^2} + \epsilon
			\]
		\item This results in the following fit:
			\begin{popblock}{0.8\textwidth}{}
				\begin{tabular}[h]{lrrrr}
					{} & {\blue Coefficient} & {\blue Std. Error} & {\blue $t$-statistic} & {\blue $p$-value} \\
					\dat{Intercept} & $56.9001$ & $1.8004$ & $31.6$ & $< 0.0001$ \\
					\dat{horsepower} & $-0.4662$ & $0.0311$ & $-15.0$ & $< 0.0001$ \\
					$\mdat{horsepower}^2$ & $0.0012$ & $0.0001$ & $10.1$ & $< 0.0001$ \\
				\end{tabular}
			\end{popblock}
	\end{itemize}
	\bl{The quadratic term improves the fit.}
\end{frame}

\begin{frame}{Non-linear Relationships}
	\vspace{-10mm}
	\begin{center}
		\includegraphics[width=0.6\textwidth]{3_8}
	\end{center}
	\vspace{-10mm}
	\bl{Remember the bias-variance trade-off.}
\end{frame}

\begin{frame}{Non-linear Relationships}
	\begin{center}
		\includegraphics[width=0.8\textwidth]{3_9}
	\end{center}
	\bl{A good way to spot non-linearities are \e{residual plots}.}
\end{frame}

\begin{frame}{Outliers}
	\vspace{-8mm}
	\begin{center}
		\includegraphics[width=0.8\textwidth]{3_12}
	\end{center}
	\vspace{-8mm}
	\begin{itemize}
		\item \e{Outliers} are data points with unusual response values $y_i$.
		\item They can be most easily spotted in residual plots.
		\item Even if they don't affect the parameters much they can badly influence\\
			statistics like $R^2$ and $p$-values.
		\item This can potentially change our interpretation of the model.
	\end{itemize}
	\bl{Be careful, only remove outliers for a good reason. In doubt report both fits.}
\end{frame}

\begin{frame}{High Leverage Points}
	\vspace{-8mm}
	\begin{center}
		\includegraphics[width=0.8\textwidth]{3_13}
	\end{center}
	\vspace{-8mm}
	\begin{itemize}
		\item \e{High leverage points} are data points with unusual predictor values $x_i$.
		\item They tend to have a strong impact on the estimated parameters.
		\item For simple linear regression the \e{leverage} is computed like this:
			\[
				h_i = \frac{1}{n} + \frac{(x_i - \overline{x})^2}{\sum^n_{i' = 1} (x_{i'} - \overline{x})^2}
			\]
	\end{itemize}
	\bl{In general, the $\bm{h_i}$ are the diagonal elements of the \e{hat matrix}.}
\end{frame}
\end{document}
