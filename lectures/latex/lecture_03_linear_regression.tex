\documentclass[mathserif, aspectratio=169]{beamer}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% need to split the includes to make spell checking work.
\input{preamble}
\subtitle{\bfseries%
  {Linear Regression}\\%
  {\tiny\it linear models, least square fit, simple \& multiple linear regression, qualitative predictors}\\%
}
\begin{document}
\input{common}

\begin{frame}{Abstract}

	\begin{blurb}
		Linear models are an important topic in statistical learning.  

		The true relationships between predictors and responses are rarely linear.
		But linear models often provide reasonable approximation. They provide
		high interpretability and have low variance, mitigating the risk of over-fitting.
		Linear models can be extended to include (some) non-linear relationships. 

		Linear models also provide an excellent baseline to compare other models against: if 
		our sophisticated model does not do much better than a linear model we might consider
		trading some bias for lower variance.
	\end{blurb}
\end{frame}

\begin{frame}{Overview}
	\begin{itemize}
		\item Simple linear regression.
		\item Multiple linear regression.
		\item Qualitative predictors.
		\item Extensions to the linear model.
	\end{itemize}
	\bottomline{This will require some mathematics.}
\end{frame}

\begin{frame}{The Advertising Data Set}
	\begin{center}
		\includegraphics[width=0.6\textwidth]{2_1}

		We want to understand how \dat{sales} depends on \dat{TV}, \dat{radio} and \dat{newspaper}.
	\end{center}
	\bottomline{We will use this data set to illustrate the concepts in this lecture.}
\end{frame}

\begin{frame}{Interesting Questions}
	\begin{enumerate}
		\item Is there a relationship between advertising budget and sales?
		\item How strong is the relationship between advertising budget and sales?
		\item Which media contribute to sales?
		\item How accurately can we estimate the effect of each medium on sales?
		\item How accurately can we predict future sales?
		\item Is the relationship linear?
		\item Is there synergy among the advertising media?
	\end{enumerate}
	\bottomline{Linear regression can answer all of these questions.}
\end{frame}

\begin{frame}{Simple Linear Regression}
	\begin{itemize}
		\item Simple linear regression assumes an approximate simple linear relationship between\\
			one predictor and the response:
			\[ Y \approx \beta_0 + \beta_1 X \]
		\item For example, $X$ might represent the \dat{TV} budget and $Y$ might represent \dat{sales}:
			\[ \text{\dat{sales}} \approx \beta_0 + \beta_1\times\text{\dat{TV}} \]
		\item The \e{coefficients}, or \e{parameters}, $\beta_0$ and $\beta_1$ are the \e{intercept} and
			\e{slope} of a line.
		\item We can \e{estimate} the parameters from the training data and \e{predict} \dat{sales}\\
			from the \dat{TV} budget.
			\[ \hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x \]
	\end{itemize}
	\bottomline{We use the \e{hat} symbol, $\;\bm{\hat{}}\;$, to denote estimates and predictions.}
\end{frame}

\begin{frame}{Estimating the Coefficients}
	\begin{itemize}
		\item In practice, $\beta_0$ and $\beta_1$ are unknown.
		\item Given the training data 
			\[ (x_1, y_1), (x_2, y_2), \dots, (x_n, y_n) \]
			we want to obtain estimates of the coefficients $\beta_0$ and $\beta_1$ such that
			\[ y_i \approx \hat{\beta}_0 + \hat{\beta}_1 x_i,\;\; \forall i = 1,\dots,n \]
		\item In other words we want to find $\hat{\beta}_0$ and $\hat{\beta}_1$ such that the
			resulting line is as close\\
			as possible to the $n = 200$ observations in the \dat{Advertising} data set.
	\end{itemize}
	\bottomline{We need to define what we mean by ``close''.}	
\end{frame}

\begin{frame}{Residual Sum of Squares}
	\begin{itemize}
		\item Given the predictions of $Y$ for the $i$th value of $X$
			\[ \hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i \]
			we define the $i$ith \e{residual} as
			\[ e_i = y_i - \hat{y}_i \]
		\item The \e{residual sum of squares} is then
			\[ \text{RSS} = e_1^2 + e_2^2 + \dots + e_n^2 \]  
			or equivalently
			\[ \text{RSS} 
				= (y_1 - \hat{\beta}_0 - \hat{\beta}_1 x_1)^2 
				+ (y_2 - \hat{\beta}_0 - \hat{\beta}_1 x_2)^2
				+ \dots
				+ (y_n - \hat{\beta}_0 - \hat{\beta}_1 x_n)^2\] 
	\end{itemize}
	\bottomline{A least squares fit minimises the RSS.}
\end{frame}

\begin{frame}{Least Squares Fit}
	\begin{itemize}
		\item The \e{least squares} approach chooses $\hat{\beta}_0$ and $\hat{\beta}_1$
			to minimise the RSS.
		\item Using some calculus and sum manipulation we can show that the optimal\\
			parameter estimates are:
				\begin{align*}
					\hat{\beta}_1 &= \frac{\sum_{i=1}^n (x_i - \overline{x})(y_i - \overline{y})}
					{\sum_{i=1}^n (x_i - \overline{x})^2}\\
					\phantom{a} & \phantom{b} \\
					\hat{\beta}_0 &= \overline{y} - \hat{\beta}_1 \overline{x}\\
				\end{align*}
	\end{itemize}
	\bottomline{Buckle up, we are going to prove it.}
\end{frame}

\begin{frame}{Least Squares Fit}
	\begin{center}
		\includegraphics[width=0.6\textwidth]{3_1}

		Result of the least squares fit for the regression of \dat{sales} onto \dat{TV}.
	\end{center}
\end{frame}

\begin{frame}{Least Squares Fit}
	\begin{center}
		\includegraphics[width=0.4\textwidth]{3_2a}
		\includegraphics[width=0.4\textwidth]{3_2b}

		The RSS with \dat{sales} as the response and \dat{TV} as the predictor. The red dot marks the optimum.
	\end{center}
\end{frame}

\begin{frame}{Assessing Accuracies}
	\begin{itemize}
		\item We want to \e{quantitatively} answer two rather important questions: 
	\end{itemize}
	\begin{cpage}
		\begin{enumerate}
			\item \e{\orange Is there a relationship between the predictor and the response?}
			\item \e{\orange To what extend does our model fit the data?}
		\end{enumerate}
	\end{cpage}
	\begin{itemize}
		\item To this end, we need to clarify a few concepts. In particular the notions of \e{population},
			\e{sample} and \e{degrees of freedom}.
		\item Then we introduce a number of so-called \e{statistics}. Simply put, these are useful quantities
			we can compute from our data.
		\item It is beyond the scope of this course to formally justify all of this, but we will now
			spend some time to develop some intuition.
	\end{itemize}
	\bottomline{For this interlude we will look at the simpler problem of estimating a mean.}
\end{frame}

\begin{frame}{Populations}
	\begin{itemize}
		\item We (somewhat sloppily) define a \e{population} as the \e{full set} of potential observations.
		\item That could be an actual population (as in everyday language) like, say, the population\\
			of \e{all} \href{https://en.wikipedia.org/wiki/Blue\_whale}{\blue\underline{blue whales}} 
			alive on Earth today.
		\item It can also be the full set of instances of any abstract entity.
		\item We then might be interested in measuring a certain property of the instances in the population.
		\item For example, the \dat{length} of all blue whales alive on Earth today.
		\item Often we want to summarise our findings rather than presenting a raw data table \\
			(or graphical representation thereof). 
		\item A common way to summarise the data is reporting the \e{mean} and the\\
			\e{standard error} of the quantity of interest.
	\end{itemize}
	\bottomline{Note that there is nothing wrong with reporting the full distribution!}
\end{frame}

\begin{frame}{Samples}
	\begin{itemize}
		\item In practice, it is generally not possible to access the whole population.
		\item Quite literally, we don't have access to \e{all} blue whales.
		\item But we can look at a \e{sample} of the full population.
		\item The sample is a subset of the population we have access to.
		\item We generally assume the sample to be a \e{random sample}.
		\item Then we can try to \e{estimate} the mean and variance of the quantity of interest\\
			from the sample.
		\item Clearly, our estimates will \e{not} yield the \e{true} values of the mean and the variance.
	\end{itemize}
	\bottomline{Our goal is to avoid any \e{bias} in the estimates and evaluate their \e{accuracy}.}
\end{frame}

\begin{frame}{Estimating the Population Mean}
	\begin{itemize}
		\item If we had access to the whole population we could calculate the \e{true} mean $\mu$.
		\item Sadly, we have only access to a sample of $n$ observations of the random variable $Y$.
		\item So all we can do is produce an estimate $\hat{\mu}$:
			\[ \hat{\mu} = \sum_{i=1}^n y_i \]
		\item This is the best possible \e{unbiased} estimate we can draw from a random sample.
		\item Intuitively, this is obvious from the fact that $\hat{\mu} = \mu$ if the sample covers
			the entire population.
	\end{itemize}
	\bottomline{It is \e{very} important that the sample is random. Beware of \e{selection bias}!}
\end{frame}

\begin{frame}{The Error of the Mean}
	\begin{itemize}
		\item Let's assume our sample was drawn from a normal (Gaussian) distribution with\\
			\e{unkown} mean $\mu$ but \e{known} variance $\sigma^2$.
		\item Then the squared \e{standard error} of our estimate $\hat{\mu}$ is:
			\[ \text{Var}(\hat{\mu}) = \text{SE}(\hat{\mu})^2 = \frac{\sigma^2}{n} \]
		\item It is then common to report our findings in the form 
			\[ \mu = \hat{\mu} \pm \frac{\sigma}{\sqrt{n}} \]
			or give a specific \e{confidence interval}.
	\end{itemize}
	\bottomline{In pratice we are rarely that lucky.}
\end{frame}

\begin{frame}{Estimating the Population Variance}
	\begin{itemize}
		\item More often than not, neither the population mean $\mu$ nor the variance $\sigma^2$ are kown.
		\item Then we have to leverage the the sample mean $\hat{\mu}$ to compute the \e{sample variance}
			\[ 
				\hat{\sigma}^2 = \langle (y_i - \hat{\mu})^2 \rangle 
				= \frac{1}{n}\sum_{i=1}^{n}  (y_i - \hat{\mu})^2
			\]
			as an estimate of the true population variance.
	\end{itemize}
	\bottomline{Unfortunately, this $\bm{\hat{\sigma}^2}$ is \e{biased} because in general $\bm{\hat{\mu} \ne \mu}$!}
\end{frame}

\begin{frame}{The Sample Variance Bias}
	\begin{columns}
		\begin{column}{0.7\textwidth}
			\begin{itemize}
				\item We have simulated a data set of blue whale \dat{length}s.
				\item The sample mean is $\hat{\mu} = 26.0$.
				\item The sample variance is:
					\[ \frac{1}{n} \sum_{i=1}^n (y_i - {\blue\hat{\mu}})^2 = 17.37 \]
				\item The \e{true} sample variance is (in general we don't have access to $\mu$): 
					\[ \frac{1}{n} \sum_{i=1}^n (y_i - {\orange \mu})^2 = 23.97 \]
			\end{itemize}
		\end{column}
		\begin{column}{0.3\textwidth}
			\begin{popblock}{0.95\textwidth}{{\blue $\hat{\mu}=26.0$}\\{\orange $\mu=22.5$}\\{\dark $n=5$}}
				\begin{tabular}[h]{lr}
					observation & \dat{length}\\
					1 & 24.23 \\
					2 & 29.57 \\
					3 & 25.14 \\
					4 & 30.10 \\
					5 & 20.97 \\
				\end{tabular}
			\end{popblock}
		\end{column}
	\end{columns}
	\bottomline{The sample variance is \e{systematically biased} to lower values.}
\end{frame}

\begin{frame}{Source of the Sample Variance Bias}
	\begin{itemize}
		\item The source of the bias is that the estimate $\hat{\mu}$ does \e{not} have $n$
			\e{degrees of freedom}.
		\item Intuitively, this can be understood from the extreme case of $n=1$.
		\item With $n=1$ the sample variance would be zero, resulting in a zero standard error.
		\item Clearly it does not make sense to assume absolute confidence in $\hat{\mu}$.
		\item We need \e{at least two observations} to even start estimating the standard error
			of $\hat{\mu}$.\\
			(If we don't know the true $\mu$, which is generally the case).
		\item We therefore apply
			\href{https://en.wikipedia.org/wiki/Bessel\%27s_correction}{\blue\underline{Bessel's correction}} 
			when computing the estimate $\hat{\sigma}^2$:
			\[ \hat{\sigma}^2 = \frac{1}{\orange n-1}\sum_{i=1}^n (y_i - \hat{\mu})^2 \]
	\end{itemize}
	\bottomline{For large $\bm{n}$ this distinction is less relevant. The idea extends to more than one parameter.}
\end{frame}

\begin{frame}{The Population Regression Line}
	\begin{itemize}
		\item Recall that the \e{true} relationship between $X$ and $Y$ takes the general form
			\[ Y = f(X) + \epsilon \]
			where $\epsilon$ is a random error term with mean zero.
		\item If we approximate $f$ as a linear function the relationship becomes:
			\[ Y = \beta_0 + \beta_1 X + \epsilon \]
		\item The error term is a catch-all for what we miss with this simplified model.
		\item This linear model defines the \e{population regression line}.
		\item This the best linear approximation to the true relationship between $X$ and $Y$.
		\item It is analogous to the \e{population mean} we discussed earlier.
	\end{itemize}
	\bottomline{Note that the true relationship is \e{not} required to be linear.}
\end{frame}

\begin{frame}{The Population Regression Line}
	\begin{center}
		\includegraphics[width=0.6\textwidth]{3_3}
	\end{center}
	\vspace{-5mm}
	\begin{itemize}
		\item We created a random sample from the model $Y = 2 + 3 X + \epsilon$.
		\item The population regression line is shown in red.
		\item \e{Sample regression lines} are shown in blue.
	\end{itemize}
	\bottomline{The mean over many samples is close to the population regression line.}
\end{frame}

\begin{frame}{Accuracy of the Coefficient Estimates}
	\begin{itemize}
		\item The \e{sample regression line} is given by 
			\[ Y = \hat{\beta_0} + \hat{\beta}_1 X \]
			with the estimated coefficients $\hat{\beta_0}$ and $\hat{\beta_1}$.
		\item The associated standard errors are then (this is a bit tedious to show):
			\[
				\text{SE}(\hat{\beta_0})^2 = \sigma^2 \left( \frac{1}{n} + \frac{\overline{x}^2}{\sum_{i=1}^n (x_i - \overline{x})^2}\right)
				,\;\;
				\text{SE}(\hat{\beta_1})^2 = \frac{\sigma^2}{\sum_{i=1}^n (x_i - \overline{x})^2}
			\]
		\item This assumes the $\epsilon_i$ are uncorrelated with variance $\sigma^2$.
	\end{itemize}
	\bottomline{In general the population variance $\bm{\sigma^2}$ is not known.}
\end{frame}

\begin{frame}{Estimating the Population Variance}
	\begin{itemize}
		\item If we don't know the population variance $\sigma^2$ we need to estimate it.
		\item This works like in the simple case of estimating a population mean.
		\item The estimated $\hat{\sigma}^2$, or \e{residual standard error}, is
			\[
				\hat{\sigma}^2 = \text{RSE} = \sqrt{\frac{1}{\orange n - 2} \text{RSS}}
				= \sqrt{\frac{1}{\orange n-2} \sum_{i=1}^n (y_i - \hat{y}_i)^2}
			\]
			with the \e{residual sum of squares}
			\[
				\text{RSS} = \sum_{i=1}^n (y_i - \hat{y}_i)^2
			\]
	\end{itemize}
	\bottomline{Note that number of degrees of freedom is now $\bm{n-2}$!}
\end{frame}

\begin{frame}{Confidence Intervals}
	\begin{itemize}
		\item Given the standard errors for the coefficient estimate, we can compute \e{confidence intervals}.
	\end{itemize}
			\begin{cpage}\orange
				A 95\% confidence interval is a range of values such that the interval will contain
				the true parameter value with a probability of 95\%.
			\end{cpage}
	\begin{itemize}
		\item This is somewhat sloppy but very practical definition.
		\item In simple linear regression the 95\% confidence interval for the parameters $\beta$ is
			approximately:
				\[ \hat{\beta} \pm 2\cdot\text{SE}(\hat{\beta}) \]
		\item That is, with 95\% probability the true value $\beta$ lies in the interval
			\[
				\left[ \hat{\beta} - 2\cdot\text{SE}(\hat{\beta}),  \hat{\beta} + 2\cdot\text{SE}(\hat{\beta})\right]
			\]
	\end{itemize}
	\bottomline{This assumes Gaussian errors and large $\bm{n}$. In practice we use computers to get it right.}
\end{frame}

\begin{frame}{Hypothesis Testing}
	\begin{itemize}
		\item We are interested in whether $X$ is related to $Y$.
		\item This question is answered by a \e{hypothesis test}.
		\item Most commonly, we want to distinguish the \e{null hypothesis}
			\[ H_0 : \beta_1 = 0 \]
			from the \e{alternative hypothesis}
			\[ H_a : \beta_1 \ne 0 \]
		\item If $\beta_1 = 0$, $Y = \beta_0 +\epsilon$ and there is no relationship between
			$X$ and $Y$.
		\item If $|\beta_1|$ is large, we can conclude that there is a relationship.
	\end{itemize}
	\bottomline{We need to quantify what we mean by ``large'' in this context.}
\end{frame}

\begin{frame}{The $\bm{t}$-statistic}
	\begin{itemize}
		\item Clearly simply checking whether $\hat{\beta}_1 \ne 0$ is not good enough.
		\item The answer must depend on our confidence in the estimated value.
		\item This confidence depends on the value of the standard error $\text{SE}(\hat{\beta}_1)$.
		\item The \e{$t$-statistic} weighs the deviation of $\hat{\beta}_1$ from zero by the
			standard error:
			\[ t = \frac{\hat{\beta}_1 - 0}{\text{SE}(\hat{\beta}_1)} \]
		\item If there is \e{truly} no relationship between $X$ and $Y$, this quantity follows a\\
			\href{https://en.wikipedia.org/wiki/Student\%27s_t-distribution}{\blue\underline{$t$-distribution}}
			with $n-2$ degrees of freedom.
	\end{itemize}
	\bottomline{The $\bm{t}$-distribution approaches the normal distribution for large $\bm{n}$.}
\end{frame}
\end{document}
