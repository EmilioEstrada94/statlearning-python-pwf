\documentclass[mathserif, aspectratio=169]{beamer}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% need to split the includes to make spell checking work.
\input{preamble}
\subtitle{\bfseries%
  {Linear Regression, Part 2}\\%
  {\tiny\it multiple linear regression, qualitative predictors, interaction \& non-linear terms, leverage and other considerations}\\%
}
\begin{document}
\input{common}

\begin{frame}{Abstract}

	\begin{blurb}
		Linear models are an important topic in statistical learning.  

		The true relationships between predictors and responses are rarely linear.
		But linear models often provide reasonable approximation. They provide
		high interpretability and have low variance, mitigating the risk of over-fitting.
		Linear models can be extended to include (some) non-linear relationships. 

		Linear models also provide an excellent baseline to compare other models against: if 
		our sophisticated model does not do much better than a linear model we might consider
		trading some bias for lower variance.
	\end{blurb}
\end{frame}

\begin{frame}{Overview}
	\begin{itemize}
		\item Linear models with multiple predictors.
		\item Qualitative predictors.
		\item Extensions to the linear model.
		\item Leverage and other considerations.
	\end{itemize}
	\bottomline{This will require some mathematics.}
\end{frame}

\begin{frame}{The Scenario}
	\begin{itemize}
		\item We look at the \dat{Advertising} data set again.
		\item Previously, we only used the \dat{TV} predictor to predict \dat{sales}.
		\item The data set provides two more predictors, \dat{radio} and \dat{newspaper}.
		\item We are now interested in using all available information in the data set.
		\item We can think of two ways of doing that:
			\begin{cpage}
				\begin{enumerate}
					\item Perform simple linear regression on the predictors separately.
					\item Somehow combine all predictors.
				\end{enumerate}
			\end{cpage}
	\end{itemize}
	\bottomline{As you might guess, option two is more interesting, but let's look at option one first.}
\end{frame}

\begin{frame}{Separate Simple Linear Regression}
	\begin{center}
		\includegraphics[width=0.7\textwidth]{2_1}
	\end{center}
	\bottomline{Separate simple linear regression looks promising on this data set.}
\end{frame}

\begin{frame}{Separate Simple Linear Regression}
	\begin{popblock}{0.7\textwidth}{Regression of \dat{sales} onto \dat{radio}}
		\begin{tabular}[h]{lrrrr}
			{} & {\blue Coefficient} & {\blue Std. Error} & {\blue $t$-statistic} & {\blue $p$-value} \\
			\dat{Intercept} & 9.312 & 0.563 & 16.54 & $< 0.0001$ \\
			\dat{radio} & 0.203 & 0.020 & 9.92 & $< 0.0001$ \\
		\end{tabular}
	\end{popblock}
	\begin{popblock}{0.7\textwidth}{Regression of \dat{sales} onto \dat{newspaper}}
		\begin{tabular}[h]{lrrrr}
			{} & {\blue Coefficient} & {\blue Std. Error} & {\blue $t$-statistic} & {\blue $p$-value} \\
			\dat{Intercept} & 12.351 & 0.621 & 19.88 & $< 0.0001$ \\
			\dat{newspaper} & 0.055 & 0.017 & 3.30 & $ 0.00115$ \\
		\end{tabular}
	\end{popblock}
	\bottomline{All the separate regressions look reasonable.}
\end{frame}

\begin{frame}{Separate Simple Linear Regression}
	\begin{itemize}
		\item This is not entirely satisfactory for several reasons.
		\item It is unclear how could predict a single value of \dat{sales} from
			the separate models.
		\item Each of the separate models ignores the other variables.
		\item This is problematic if there are correlations between the predictors.
	\end{itemize}
	\bottomline{A better approach is to use multiple predictors simultaneously.}
\end{frame}


\begin{frame}{Multiple Simple Linear Regression}
	\begin{itemize}
		\item \e{Multiple linear regression} uses multiple predictors.
		\item Each predictor has its own slope coefficient.
		\item For $p$ predictors the model takes the form
			\[ Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p + \epsilon \]
			where the $\beta_j$ are interpreted as the average effect $X_j$ has on $Y$ while\\
			\e{holding all other predictors constant}.
		\item For example:
			\[ \text{\dat{sales}} = \beta_0
				+ \beta_1\times\text{\dat{TV}}
				+ \beta_2\times\text{\dat{radio}}
				+ \beta_3\times\text{\dat{newspaper}}
				+ \epsilon
			\]
	\end{itemize}
	\bottomline{Now we have one model using all the information.}
\end{frame}

\begin{frame}{Example: Two Predictors}
	\begin{center}
		\vspace{-5mm}
		\includegraphics[width=0.5\textwidth]{3_4}

		\vspace{-8mm}
		\[ Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon \]
	\end{center}
	\bl{In general, the result is a \e{hyperplane}.}
\end{frame}

\begin{frame}{Estimating the Regression Coefficients}
	\begin{itemize}
		\item As in simple linear regression we want to make predictions using
			estimated parameters:
			\[ \h{y} = \bh_0 + \bh_1 x_1 + \bh_2 x_2 + \dots + \bh_p x_p \]
		\item We choose the coefficients $\bh_0, \bh_1, \bh_2, \dots, \bh_p$
			to minimise the sum of squared residuals:
			\begin{align*}
				\text{RSS} &= \sum_{i=1}^n (y_i - \hat{y}_i)^2 \\
				{} &= \sum_{i=1}^{n} (y_i - \bh_0 - \bh_1 x_1 - \bh_2 x_2 - \dots - \bh_p x_p)^2
			\end{align*}
	\end{itemize}
	\bl{Deriving the solution involves some matrix manipulations.}
\end{frame}
\end{document}
