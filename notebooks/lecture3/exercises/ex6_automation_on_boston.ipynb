{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction to Statistical Learning, Exercise 3.6\n",
    "\n",
    "__Please do yourself a favour and only look at the solutions after you honestly tried to solve the exercises.__\n",
    "\n",
    "# Boston Data Set, Automation\n",
    "\n",
    "We will further investigate the `Boston` data set, trying to predict the crime rate. We are going to fit a lot of models and compare the. This requires some automation, ie. writing some loops.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from islpy import datasets, utils, lmplots\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Regression for each Predictor\n",
    "\n",
    "For each predictor, fit a *simple linear regression* model with `crim` as the response. Describe your results. In which of the models do you find a statistically significant relationship between the predictor and the response? Produce some plots to back up your assertions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = datasets.Boston()\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lms = {}\n",
    "for pred in boston.columns.drop('crim'):\n",
    "    lms[pred] = smf.ols(f'crim~{pred}', boston).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = [lm.pvalues[1] for lm in lms.values()]\n",
    "fs = [lm.fvalue for lm in lms.values()]\n",
    "coeff = [lm.params[1] for lm in lms.values()]\n",
    "err = [lm.bse[1] for lm in lms.values()]\n",
    "\n",
    "results = pd.DataFrame({'p_value': ps, 'f_stat': fs, 'coeff': coeff, 'err': err},\n",
    "                       index=lms.keys())\n",
    "results.sort_values(by='p_value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the $p$-values, we can reject the null hypothesis $H_0: \\beta_1 = 0$ for all but one (`chas`) of the models.\n",
    "\n",
    "As expected in this scenario, we observe high $F$-statistic values for low $p$-values.\n",
    "\n",
    "As examples, we plot the fit results of the `lstat` and `nox` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4.5))\n",
    "lmplots.plot_fit(lms['lstat'], 'lstat', ax=ax[0], show_pi=True, lowess=True, legend=True)\n",
    "lmplots.plot_fit(lms['nox'], 'nox', ax=ax[1], show_pi=True, lowess=True, legend=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
