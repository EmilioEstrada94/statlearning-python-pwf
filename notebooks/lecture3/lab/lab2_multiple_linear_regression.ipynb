{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction to Statistical Learning, Lab 3.2\n",
    "\n",
    "# Multiple Linear Regression\n",
    "\n",
    "In the Python environment the most popular libraries for model fitting (and therefore linear regression) *sklearn* and *statsmodels*. The statsmodels library provides a R-style formula-based interface. We will mostly use this interface because it provides more flexibility and better parameter reporting. This has the additional advantage that it maps quite well onto the examples in the ISLR book.  \n",
    "\n",
    "\n",
    "  - [statsmodels documentation](https://www.statsmodels.org/stable/)\n",
    "  - [statsmodels formula interface](https://www.statsmodels.org/stable/example_formulas.html)\n",
    "  - [the formula mini language](https://patsy.readthedocs.io/en/latest/formulas.html#the-formula-language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "from islpy import datasets\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Set\n",
    "\n",
    "We use the `Boston` data set to demonstrate multiple linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = datasets.Boston()\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Specification\n",
    "\n",
    "The `smf.ols()` function builds a statistical *model* prepared for fitting with *ordinary least squares* (ols). This is the type of fit explained in detail in the lecture.\n",
    "\n",
    "the syntax to use multiple regressors (variables, predictors, features...) is `y~x1+x2+x3`. As in the simple regression with one predictor, a constant term for the intercept is added automatically.\n",
    "\n",
    "The formula `medv~lstat+age` means we are using `lstat` and `age` as our predictors and `medv` as our dependent variable:\n",
    "\n",
    "$$ \\mathrm{medv} = \\beta_0 + \\beta_1 \\mathrm{lstat} + \\beta_2 \\mathrm{age}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.ols(formula='medv~lstat+age', data=boston)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the Model\n",
    "\n",
    "We *fit* the model to the data by calling the `fit()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit = model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit Result Summary\n",
    "\n",
    "We can get a comprehensive summary using the `summary()` method. Now we get the results for all three $\\beta$ coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specific Summary Tables\n",
    "\n",
    "We can also select a specific table from the summary. For example the fitted coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit Result Parameters\n",
    "\n",
    "Or we can retrieve only the fitted parameters ($\\beta_0$ = *intercept*, $\\beta_1$ = *lstat*) as a pandas series using the `params` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confidence Intervals\n",
    "\n",
    "The 95% confidence intervals for the coefficients can be retrieved via the `conf_int()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit.conf_int()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualising the Fit Results\n",
    "\n",
    "With two predictors we can visualise the data and the fit result in a 3D plot. The `seaborn` library does not provide 3D plotting facilities. There is a good reason for that: it is very hard to make informative 3D charts. Most of the time it is much better to think of a good way to visualise the data in 2D.\n",
    "\n",
    "That said, we want to give at least one example of a 3D chart. Our approach is similar to the one variable case:\n",
    "\n",
    "  - First produce a 3D scatter plot.\n",
    "  - Next get a range of predictor values from the plot's x- and y-axis and compute all predictions on a 2D grid.\n",
    "  - Then use the `plot_surface()` method to overlay the prediction plane of the fitted model.\n",
    "  \n",
    "Like in the one variable case, this approach might seem a bit heavy-handed for a linear model (it plots surface segments between many points on the grid, while only very few are necessary). But again it does have the advantage that it works with *any* model!\n",
    "\n",
    "In particular this also works for the confidence level surfaces which are *not* planes.\n",
    "\n",
    "What follows is quite a bit of code; making reasonably looking 3D plots is a bit of work. We include a number of different features in this chart so you have a reference to come back to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from matplotlib import cm\n",
    "\n",
    "fig = plt.figure(figsize=(12, 9))\n",
    "ax = axes3d.Axes3D(fig)\n",
    "\n",
    "# 3D scatter plot of the raw data\n",
    "ax.scatter(boston.lstat, boston.age, boston.medv)\n",
    "\n",
    "# prepare point grids from the ranges of the scatter plot\n",
    "xs = np.linspace(*ax.get_xlim(), 100)\n",
    "ys = np.linspace(*ax.get_ylim(), 100)\n",
    "xv, yv = np.meshgrid(xs, ys, copy=False)\n",
    "zv = np.zeros((ys.size, xs.size))\n",
    "lv = np.zeros((ys.size, xs.size))\n",
    "uv = np.zeros((ys.size, xs.size))\n",
    "\n",
    "# compute predictions and CI bounds for the rows in the point grids\n",
    "for idx, y in enumerate(yv):\n",
    "    pred = model_fit.get_prediction({'lstat': xs, 'age': y}).summary_frame()\n",
    "    zv[idx] = pred['mean']\n",
    "    lv[idx] = pred['mean_ci_lower']\n",
    "    uv[idx] = pred['mean_ci_upper']\n",
    "\n",
    "# plot the prediction & CI boundary surfaces\n",
    "ax.plot_surface(xv, yv, zv, alpha=0.4)\n",
    "ax.plot_surface(xv, yv, lv, alpha=0.2, color='C1')\n",
    "ax.plot_surface(xv, yv, uv, alpha=0.2, color='C1')\n",
    "\n",
    "# add contour plot of the CI width to the bottom of the figure\n",
    "ax.contourf(xv, yv, uv-lv,\n",
    "            zdir='z',\n",
    "            offset=ax.get_zlim()[0],\n",
    "            levels=30,\n",
    "            antialiased=True,\n",
    "            cmap=cm.Oranges)\n",
    "\n",
    "# set figure title and axes labels\n",
    "ax.set_title('Linear regression on Boston housing data: medv ~ lstat + age')\n",
    "ax.set_xlabel('lstat')\n",
    "ax.set_ylabel('age')\n",
    "ax.set_zlabel('medv')\n",
    "\n",
    "# specify viewing angle\n",
    "ax.view_init(15, -70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks cool, but that's a bad reason to do it. Choosing a good viewing angle and the right surface transparencies can be a bit tricky. You can waste a lot of time (and CPU cycles) on this kind of thing. \n",
    "\n",
    "Obviously, once we use more than two predictors, this approach to visualisation won't work anymore. You can play tricks with colour coding and so on. But you can do that in 2D as well and the results will be much more readable!\n",
    "\n",
    "The bottom line is: __*don't make 3D charts unless you absolutely have to*__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
