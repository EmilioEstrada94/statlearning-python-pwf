{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction to Statistical Learning, Lab 4.5\n",
    "\n",
    "# K-Nearest Neighbours\n",
    "\n",
    "\n",
    "We will now perform a KNN classification analysis on the `Smarket` data set, trying to predict `Direction` using `Lag1` and `Lag2`. We again use the `sklearn` library.\n",
    "\n",
    "The KNN classifier relies on the distances between the predictors. This raises the question what the proper distances are. If we measured everything in the same units (say, metres) that would not be an issue. \n",
    "\n",
    "But we are often faced with data sets containing predictors such as `income` (measured in thousands of dollars) and `age` (measured in years).\n",
    "\n",
    "Intuitively, we know that a difference of 50 years is more important than an income difference of $1000.  \n",
    "\n",
    "The computer does not know that, though. We therefore have to *normalise* our data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "import patsy\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from islpy import datasets, utils, lmplots\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smarket = datasets.Smarket()\n",
    "smarket.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the observations before 2005 as the training sample and the later predictions as the test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = smarket[smarket.Year < 2005][['Lag1', 'Lag2']]\n",
    "Y_train = smarket[smarket.Year < 2005]['Direction']\n",
    "X_test = smarket[smarket.Year == 2005][['Lag1', 'Lag2']]\n",
    "Y_test = smarket[smarket.Year == 2005]['Direction']\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first train the scaling on our training data set. We use the `StandardScaler` from `sklearn`. This will scale our predictors to a distribution with mean 0 and standard deviation of 1. \n",
    "\n",
    "It is *very important* to use the same scaling on the test data, just like with any other model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = scaler.transform(X_train)\n",
    "x_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the training and test data properly scaled, we are now ready to fit a KNN classifier. \n",
    "\n",
    "We first look at a KNN classifier with $k=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn1 = KNeighborsClassifier(1)\n",
    "knn1_fit = knn1.fit(x_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = knn1_fit.predict(x_test)\n",
    "confusion_matrix(pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are rather poor: only about 50% of respones are predicted correctly, not any better than random chance.\n",
    "\n",
    "Remember that $k=1$ is a model with high flexibility. So we expect low bias and high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the KNN classifier with $k=1$. We use the same utility functions as in the previous labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=x_test[:, 0], y=x_test[:, 1], hue=Y_test)\n",
    "ax = utils.plot_decision_contour(x_test[:, 0], x_test[:, 1],\n",
    "                                 knn1_fit.predict_proba, ax=ax)\n",
    "ax = utils.plot_decision_boundaries(x_test[:, 0], x_test[:, 1],\n",
    "                                   knn1_fit.predict_proba, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now try a KNN classifier with $k=3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn3 = KNeighborsClassifier(3)\n",
    "knn3_fit = knn3.fit(x_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = knn3_fit.predict(x_test)\n",
    "confusion_matrix(pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are still poor. It turns out increasing $k$ further does not improve the situation. Feel free to explore! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the KNN classifier with $k=3$. We use the same utility functions as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=x_test[:, 0], y=x_test[:, 1], hue=Y_test)\n",
    "ax = utils.plot_decision_contour(x_test[:, 0], x_test[: ,1],\n",
    "                                 knn3_fit.predict_proba, ax=ax)\n",
    "ax = utils.plot_decision_boundaries(x_test[:, 0], x_test[:, 1],\n",
    "                                   knn3_fit.predict_proba, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this particular problem, out of all the methods we tried so far, QDA seems to perform best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
