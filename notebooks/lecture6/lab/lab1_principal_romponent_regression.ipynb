{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction to Statistical Learning, Lab 6.1\n",
    "\n",
    "# Principal Component Regression\n",
    "\n",
    "Principal components regression (PCR) can be performed using the `PCA()` function, which is part of the `sklearn` library. In this lab, we'll apply PCR to the `Hitters` data, in order to predict `Salary`. As in previous labs, we'll start by ensuring that the missing values have been removed from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn import model_selection\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_decomposition import PLSRegression, PLSSVD\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from islpwf import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As in previous labs, we'll start by ensuring that the missing values have been removed from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = datasets.Hitters().dropna().drop('Unnamed: 0', axis=1)\n",
    "df.info()\n",
    "dummies = pd.get_dummies(df[['League', 'Division', 'NewLeague']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.Salary\n",
    "\n",
    "# Drop the column with the independent variable (Salary), and columns for which we created dummy variables\n",
    "X_ = df.drop(['Salary', 'League', 'Division', 'NewLeague'], axis=1).astype('float64')\n",
    "\n",
    "# Define the feature set X.\n",
    "X = pd.concat([X_, dummies[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by performing Principal Components Analysis (PCA), remembering to scale the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_reduced = pca.fit_transform(scale(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the first few variables of the first few principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pca.components_.T).loc[:4,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll perform 10-fold cross-validation to see how it influences the MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-fold CV, with shuffle\n",
    "n = len(X_reduced)\n",
    "kf_10 = model_selection.KFold( n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "regr = LinearRegression()\n",
    "mse = []\n",
    "\n",
    "# Calculate MSE with only the intercept (no principal components in regression)\n",
    "score = -1*model_selection.cross_val_score(regr, np.ones((n,1)), y.ravel(), cv=kf_10, scoring='neg_mean_squared_error').mean()    \n",
    "mse.append(score)\n",
    "\n",
    "# Calculate MSE using CV for the 19 principle components, adding one component at the time.\n",
    "for i in np.arange(1, 20):\n",
    "    score = -1*model_selection.cross_val_score(regr, X_reduced[:,:i], y.ravel(), cv=kf_10, scoring='neg_mean_squared_error').mean()\n",
    "    mse.append(score)\n",
    "    \n",
    "# Plot results    \n",
    "plt.plot(mse, '-v')\n",
    "plt.xlabel('Number of principal components in regression')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Salary')\n",
    "plt.xlim(xmin=-1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the smallest cross-validation error occurs when  $M=18$  components are used. This is barely fewer than  $M=19$ , which amounts to simply performing least squares, because when all of the components are used in PCR no dimension reduction occurs. However, from the plot we also see that the cross-validation error is roughly the same when only one component is included in the model. This suggests that a model that uses just a small number of components might suffice.\n",
    "\n",
    "We'll do a little math to get the amount of variance explained by adding each consecutive principal component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll dig deeper into this concept in Chapter 10, but for now we can think of this as the amount of information about the predictors or the response that is captured using  $M$  principal components. For example, setting  $M=1$  only captures $38.31\\%$ of all the variance, or information, in the predictors. In contrast, using  $M=6$  increases the value to $88.63\\%$. If we were to use all  $M=p=19$  components, this would increase to $100\\%$.\n",
    "\n",
    "Now let's perform PCA on the training data and evaluate its test set performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca2 = PCA()\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test , y_train, y_test = model_selection.train_test_split(X, y, test_size=0.5, random_state=1)\n",
    "\n",
    "# Scale the data\n",
    "X_reduced_train = pca2.fit_transform(scale(X_train))\n",
    "n = len(X_reduced_train)\n",
    "\n",
    "# 10-fold CV, with shuffle\n",
    "kf_10 = model_selection.KFold( n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "mse = []\n",
    "\n",
    "# Calculate MSE with only the intercept (no principal components in regression)\n",
    "score = -1*model_selection.cross_val_score(regr, np.ones((n,1)), y_train.ravel(), cv=kf_10, scoring='neg_mean_squared_error').mean()    \n",
    "mse.append(score)\n",
    "\n",
    "# Calculate MSE using CV for the 19 principle components, adding one component at the time.\n",
    "for i in np.arange(1, 20):\n",
    "    score = -1*model_selection.cross_val_score(regr, X_reduced_train[:,:i], y_train.ravel(), cv=kf_10, scoring='neg_mean_squared_error').mean()\n",
    "    mse.append(score)\n",
    "\n",
    "plt.plot(np.array(mse), '-v')\n",
    "plt.xlabel('Number of principal components in regression')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Salary')\n",
    "plt.xlim(xmin=-1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the lowest cross-validation error occurs when  $M=6$  components are used. Now we'll see how it performs on the test data and compute the test MSE as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced_test = pca2.transform(scale(X_test))[:,:7]\n",
    "\n",
    "# Train regression model on training data \n",
    "regr = LinearRegression()\n",
    "regr.fit(X_reduced_train[:,:7], y_train)\n",
    "\n",
    "# Prediction with test data\n",
    "pred = regr.predict(X_reduced_test)\n",
    "mean_squared_error(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way PCR is implemented, the final model can be difficult to interpret because it does not perform any kind of variable selection or even directly produce coefficient (as in $\\pmb{\\beta}$) estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
