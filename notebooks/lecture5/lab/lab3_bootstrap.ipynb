{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction to Statistical Learning, Lab 5.3\n",
    "\n",
    "# Bootstrap\n",
    "\n",
    "The main advantage of the bootstrap is its wide range of applications. We first demonstrate how to use it with a custom statistic on the `Portfolio` data set and then evaluate linear models on the `Auto` data set.\n",
    "\n",
    "We will use the linear models and tools from the `sklearn` library in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.linear_model as skl_lm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from islpy import datasets, utils, lmplots\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bootstrap on Portfolio\n",
    "\n",
    "We use $\\alpha$ from the lecture as the statistic and define a function to calculate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha(x, y):\n",
    "    cv = np.cov(x, y)\n",
    "    vx = cv[0, 0]\n",
    "    vy = cv[1, 1]\n",
    "    cxy = cv[1, 0]\n",
    "    return (vy - cxy) / (vx + vy - 2 * cxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio = datasets.Portfolio()\n",
    "x = portfolio.X[:100]\n",
    "y = portfolio.Y[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.57583207459283\n"
     ]
    }
   ],
   "source": [
    "print(alpha(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now sample a bootstrap data set with replacement and compute $\\hat{\\alpha}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "xs, ys = resample(x, y, n_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(alpha(xs, ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform a bootstrap analysis we have to re-sample many times and compute the standard error. We define a function to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(x, y, r, fn):\n",
    "    stats = np.zeros(r)\n",
    "    for i in range(r):\n",
    "        xs, ys = resample(x, y, n_samples=x.shape[0])\n",
    "        stats[i] = fn(xs, ys)\n",
    "    \n",
    "    stat = np.mean(stats)\n",
    "    se = np.sqrt((r * np.var(stats)) / (r - 1))\n",
    "    \n",
    "    return stat, se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, se_a = bootstrap(x, y, 1000, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'alpha: {a:.2f}, SE(alpha): {se_a:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bootstrap on Linear Model\n",
    "\n",
    "We now perform a bootstrap analysis on the parameters of a linear regression model and compare the results to statistics produced by the regression fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto = datasets.Auto()\n",
    "x = auto[['horsepower']]\n",
    "y = auto['mpg']\n",
    "model = skl_lm.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again define a function that re-samples the data set many times and estimates the parameters and their standard errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_lm(x, y, model, r):\n",
    "    n_coeff = x.shape[1]\n",
    "    params = np.zeros((r, n_coeff + 1))\n",
    "    for i in range(r):\n",
    "        xs, ys = resample(x, y, n_samples=x.shape[0])\n",
    "        lm = model.fit(xs, ys)\n",
    "        params[i, 0] = lm.intercept_\n",
    "        params[i, 1:] = lm.coef_\n",
    "    \n",
    "    betas = np.zeros(n_coeff + 1)\n",
    "    errors = np.zeros(n_coeff + 1)\n",
    "    for i in range(n_coeff + 1):\n",
    "        betas[i] = np.mean(params[:, i])\n",
    "        errors[i] = np.sqrt((r * np.var(params[:, i])) / (r - 1))\n",
    "    \n",
    "    return betas, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_lm(x, y, model, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{\\beta}_0 = 39.94$, $\\text{SE}(\\hat{\\beta}_0) = 0.85$ \n",
    "\n",
    "$\\hat{\\beta}_1 = -0.1580$, $\\text{SE}(\\hat{\\beta}_1) = 0.0074$ \n",
    "\n",
    "(The results may vary slightly due to the random sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compare this to the results obtained from the `statsmodels` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "lm = smf.ols('mpg~horsepower', auto).fit()\n",
    "lm.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error estimates are different. Is this a problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the above with a model including a quadratic term. The `bootstrap_lm()` function we defined above can digest arbitrary linear models, so we can reuse it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "x_train = poly.fit_transform(x)\n",
    "bootstrap_lm(x_train, y, model, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{\\beta}_0 = 56.97$, $\\text{SE}(\\hat{\\beta}_0) = 2.00$ \n",
    "\n",
    "$\\hat{\\beta}_1 = -4.6739$, $\\text{SE}(\\hat{\\beta}_1) = 0.0318$ \n",
    "\n",
    "$\\hat{\\beta}_2 = -0.0012$, $\\text{SE}(\\hat{\\beta}_2) = 0.0002$ \n",
    "\n",
    "(The results may vary slightly due to the random sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = smf.ols('mpg~horsepower+I(horsepower**2)', auto).fit()\n",
    "lm.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there is a much better agreement on the errors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
